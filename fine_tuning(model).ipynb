{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8622081,"sourceType":"datasetVersion","datasetId":5161395},{"sourceId":8622086,"sourceType":"datasetVersion","datasetId":5161400},{"sourceId":8693258,"sourceType":"datasetVersion","datasetId":5213085}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import (TextDataset, DataCollatorForLanguageModeling,GPT2Tokenizer,\n                          GPT2LMHeadModel,Trainer, TrainingArguments, get_linear_schedule_with_warmup)\nfrom torch.optim import AdamW\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport wandb\nwandb.init(mode=\"disabled\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T11:42:51.460370Z","iopub.execute_input":"2024-06-15T11:42:51.460648Z","iopub.status.idle":"2024-06-15T11:43:11.596959Z","shell.execute_reply.started":"2024-06-15T11:42:51.460623Z","shell.execute_reply":"2024-06-15T11:43:11.596047Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-15 11:42:58.843345: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-15 11:42:58.843441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-15 11:42:58.977055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# df1 = pd.read_csv('/kaggle/input/train-dataset-chatbot/train_data_chatbot.csv')\n# df1 = df1.drop(['tags', 'label'], axis=1)\n# df1.to_csv('updated_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.598853Z","iopub.execute_input":"2024-06-15T11:43:11.599561Z","iopub.status.idle":"2024-06-15T11:43:11.604031Z","shell.execute_reply.started":"2024-06-15T11:43:11.599524Z","shell.execute_reply":"2024-06-15T11:43:11.602976Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# def load_dataset(file_path, tokenizer, block_size = 1024):\n#     dataset_train = TextDataset(\n#         tokenizer = tokenizer,\n#         file_path = file_path,\n#         block_size = block_size,\n#     )\n#     return dataset_train\n\n# def load_data_collator(tokenizer, mlm = False):\n#     data_collator = DataCollatorForLanguageModeling(\n#         tokenizer=tokenizer, \n#         mlm=mlm,\n#     )\n#     return data_collator","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.605562Z","iopub.execute_input":"2024-06-15T11:43:11.605831Z","iopub.status.idle":"2024-06-15T11:43:11.624167Z","shell.execute_reply.started":"2024-06-15T11:43:11.605809Z","shell.execute_reply":"2024-06-15T11:43:11.623322Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n#           per_device_train_batch_size, num_train_epochs):\n#     tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n#     # Load datasets\n#     train_dataset = load_dataset(train_file_path, tokenizer)\n\n#     # Load data collator\n#     data_collator = load_data_collator(tokenizer)\n\n#     # Save tokenizer\n#     tokenizer.save_pretrained(output_dir)\n\n#     # Load or initialize model\n#     model = GPT2LMHeadModel.from_pretrained(model_name)\n\n#     # Save model\n#     model.save_pretrained(output_dir)\n\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         overwrite_output_dir=overwrite_output_dir,\n#         num_train_epochs=num_train_epochs,\n#         warmup_steps= 500,\n#         per_device_train_batch_size=per_device_train_batch_size,\n#         logging_dir=\"./logs\",\n#         logging_steps=100,  # Log every 100 steps\n#         save_steps=500,  # Save checkpoint every 500 steps\n#         logging_first_step=True,\n#         gradient_accumulation_steps=4,\n#         save_total_limit=2,\n#         learning_rate=.0001\n#     )\n    \n#     # Define the optimizer\n#     optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n\n#     # Define the scheduler\n#     scheduler = get_linear_schedule_with_warmup(\n#         optimizer,\n#         num_warmup_steps=training_args.warmup_steps,\n#         num_training_steps=training_args.num_train_epochs * len(train_dataset) // training_args.gradient_accumulation_steps\n#     )\n\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         data_collator=data_collator,\n#         train_dataset=train_dataset,\n#         optimizers=(optimizer, scheduler)\n#     )\n#     hist = trainer.train()\n#     trainer.save_model()\n#     return trainer,hist","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.626527Z","iopub.execute_input":"2024-06-15T11:43:11.627102Z","iopub.status.idle":"2024-06-15T11:43:11.634824Z","shell.execute_reply.started":"2024-06-15T11:43:11.627071Z","shell.execute_reply":"2024-06-15T11:43:11.633949Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n#           per_device_train_batch_size, num_train_epochs):\n#     tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n#     # Load datasets\n#     train_dataset = load_dataset(train_file_path, tokenizer)\n\n#     # Load data collator\n#     data_collator = load_data_collator(tokenizer)\n\n#     # Save tokenizer\n#     tokenizer.save_pretrained(output_dir)\n\n#     # Load or initialize model\n#     model = GPT2LMHeadModel.from_pretrained(model_name)\n\n#     # Save model\n#     model.save_pretrained(output_dir)\n\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         overwrite_output_dir=overwrite_output_dir,\n#         num_train_epochs=num_train_epochs,\n#         warmup_steps= 500,\n#         per_device_train_batch_size=per_device_train_batch_size,\n#         logging_dir=\"./logs\",\n#         logging_steps=100,  # Log every 100 steps\n#         save_steps=500,  # Save checkpoint every 500 steps\n#         logging_first_step=True,\n#         gradient_accumulation_steps=4,\n#         save_total_limit=2,\n#         learning_rate=.0001\n#     )\n    \n# #     Define the optimizer\n#     optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n\n#     # Define the scheduler\n#     scheduler = get_linear_schedule_with_warmup(\n#         optimizer,\n#         num_warmup_steps=training_args.warmup_steps,\n#         num_training_steps=training_args.num_train_epochs * len(train_dataset) // training_args.gradient_accumulation_steps\n#     )\n    \n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         data_collator=data_collator,\n#         train_dataset=train_dataset,\n#         optimizers=(optimizer, scheduler)\n#     )\n#     hist = trainer.train()\n#     trainer.save_model()\n#     return trainer,hist","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.636081Z","iopub.execute_input":"2024-06-15T11:43:11.636440Z","iopub.status.idle":"2024-06-15T11:43:11.647333Z","shell.execute_reply.started":"2024-06-15T11:43:11.636411Z","shell.execute_reply":"2024-06-15T11:43:11.646521Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train_file_path = \"/kaggle/working/updated_train.csv\"\n# model_name = 'gpt2'\n# output_dir = '/kaggle/working/custom_model'\n# overwrite_output_dir = True\n# per_device_train_batch_size = 4\n# num_train_epochs = 50","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.648209Z","iopub.execute_input":"2024-06-15T11:43:11.648482Z","iopub.status.idle":"2024-06-15T11:43:11.659343Z","shell.execute_reply.started":"2024-06-15T11:43:11.648461Z","shell.execute_reply":"2024-06-15T11:43:11.658495Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n# tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.660289Z","iopub.execute_input":"2024-06-15T11:43:11.660555Z","iopub.status.idle":"2024-06-15T11:43:11.670545Z","shell.execute_reply.started":"2024-06-15T11:43:11.660525Z","shell.execute_reply":"2024-06-15T11:43:11.669779Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# model = GPT2LMHeadModel.from_pretrained(model_name)\n# model","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.671627Z","iopub.execute_input":"2024-06-15T11:43:11.671942Z","iopub.status.idle":"2024-06-15T11:43:11.678844Z","shell.execute_reply.started":"2024-06-15T11:43:11.671899Z","shell.execute_reply":"2024-06-15T11:43:11.678077Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# model.base_model","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.680195Z","iopub.execute_input":"2024-06-15T11:43:11.680472Z","iopub.status.idle":"2024-06-15T11:43:11.687361Z","shell.execute_reply.started":"2024-06-15T11:43:11.680450Z","shell.execute_reply":"2024-06-15T11:43:11.686473Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # Train\n# hist=train(\n#     train_file_path=train_file_path,\n#     model_name=model_name,\n#     output_dir=output_dir,\n#     overwrite_output_dir=overwrite_output_dir,\n#     per_device_train_batch_size=per_device_train_batch_size,\n#     num_train_epochs=num_train_epochs,\n# )    ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.690210Z","iopub.execute_input":"2024-06-15T11:43:11.690493Z","iopub.status.idle":"2024-06-15T11:43:11.696478Z","shell.execute_reply.started":"2024-06-15T11:43:11.690452Z","shell.execute_reply":"2024-06-15T11:43:11.695662Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/input/custom-model/custom_model\"\ndef load_model(model_path):\n    model = GPT2LMHeadModel.from_pretrained(model_path)\n    return model\n\n\ndef load_tokenizer(tokenizer_path):\n    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n\ndef generate_text(model_path, sequence, max_length):\n    \n    model = load_model(model_path)\n    tokenizer = load_tokenizer(model_path)\n    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n    final_outputs = model.generate(\n        ids,\n        do_sample=True,\n        max_length=max_length,\n        pad_token_id=model.config.eos_token_id,\n        top_k=50,\n        top_p=0.7,\n        temperature=0.5\n    )\n    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.697466Z","iopub.execute_input":"2024-06-15T11:43:11.698085Z","iopub.status.idle":"2024-06-15T11:43:11.706434Z","shell.execute_reply.started":"2024-06-15T11:43:11.698049Z","shell.execute_reply":"2024-06-15T11:43:11.705685Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/validation-dataset/validation_data_chatbot.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:11.707393Z","iopub.execute_input":"2024-06-15T11:43:11.707683Z","iopub.status.idle":"2024-06-15T11:43:11.923702Z","shell.execute_reply.started":"2024-06-15T11:43:11.707660Z","shell.execute_reply":"2024-06-15T11:43:11.922642Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#model_path = \"/kaggle/working/custom_model\"\n# sequence = df['short_question'].iloc[0]\nsequence = \"how do i know i have diabetes\"\nmax_len = 200\nprint(\"Q : \",df['short_question'].iloc[400])\nprint()\nprint(\"A : \",df['short_answer'].iloc[400])\nprint()\nprint(\"G : \",generate_text(model_path, sequence, max_len))","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:42.960627Z","iopub.execute_input":"2024-06-15T11:43:42.961327Z","iopub.status.idle":"2024-06-15T11:43:50.837476Z","shell.execute_reply.started":"2024-06-15T11:43:42.961290Z","shell.execute_reply":"2024-06-15T11:43:50.836519Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Q :  will baby have type i diabetes if i have gestational diabetes i have just been diagnosed with gestational diabetes the doctor said i will need to take insulin i am worried my baby will be born with type i diabetes what are the risks to my baby i am 43 weeks along\n\nA :  children born to mothers with gestational diabetes mellitus gdm are at a higher risk for obesity type 2 diabetes and metabolic syndrome but your child will not be at a higher risk for type 1 diabetes it is important to follow your doctor is recommended course of action for managing gestational diabetes to help minimize the potential risks which may include • maintaining appropriate weight gain • following specific dietary guidelines • following your doctor is recommendations for an appropriate exercise routine • monitoring your blood sugar levels frequently • taking insulin as directed blood sugar levels will typically return to normal for you after 6 to 8 weeks after delivery but you will be more likely to develop gestational diabetes again during subsequent pregnancies please refer to the following for more information <link> <link> no\n\nG :  how do i know i have diabetes,if you have diabetes and you are concerned about it talk to your doctor here are some tips eat a healthy diet that is low in refined carbohydrates white bread white rice crackers or brown rice eat lots of fruits and vegetables eat lots of cereals breads rice and pasta made from whole grains such as whole wheat bread white rice brown rice or spaghetti get plenty of exercise eat regular exercise to help control your body is use of insulin exercise may also help lower blood sugar talk to your doctor about other ways to manage your diabetes diet avoid triggers you may want to change your diet to avoid foods that cause symptoms such as stress and obesity avoid alcohol and drugs that cause high blood sugar avoid caffeine and nicotine quit smoking if you smoke quit stop taking sugar pills if you take insulin treatment ask your doctor or pharmacist how much insulin you should take each day some people need more insulin than others your doctor may prescribe a medicine to bring your blood sugar down talk to your doctor about what he or\n","output_type":"stream"}]},{"cell_type":"code","source":"# sequence = \"had a stroke on the brain in 2012 its 2016 i cant get no more than 5 hours of sleep a day\"\n# max_len = 500\n# print(\"Q : \",sequence)\n# print()\n# print(\"G : \",generate_text(model_path, sequence, max_len)) ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T11:43:23.212721Z","iopub.execute_input":"2024-06-15T11:43:23.213050Z","iopub.status.idle":"2024-06-15T11:43:23.216947Z","shell.execute_reply.started":"2024-06-15T11:43:23.213024Z","shell.execute_reply":"2024-06-15T11:43:23.215998Z"},"trusted":true},"execution_count":14,"outputs":[]}]}